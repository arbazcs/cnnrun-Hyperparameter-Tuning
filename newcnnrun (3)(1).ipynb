{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "226a5e87-cd6c-4b47-b03e-cf485a7333fd",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'skimage'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mPIL\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Image, ImageEnhance, ImageFilter\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mskimage\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfilters\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m sobel, gaussian\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mskimage\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexposure\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m equalize_adapthist\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mcm\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'skimage'"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "from PIL import Image, ImageEnhance, ImageFilter\n",
    "import os\n",
    "from skimage.filters import sobel, gaussian\n",
    "from skimage.exposure import equalize_adapthist\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "# Function to generate and save the heatmap image from row data\n",
    "def generate_image_from_row(row_dict, output_folder_path):\n",
    "    # Get the image name from 'family' and 'hash' keys\n",
    "    family = row_dict.get('family', 'unknown')\n",
    "    file_hash = row_dict.get('hash', 'nohash')\n",
    "    image_name = f\"{family}_{file_hash}.png\"\n",
    "\n",
    "    # Extract only numeric features\n",
    "    features = [float(value) for key, value in row_dict.items()\n",
    "                if key not in ['hash', 'family'] and str(value).replace('.', '', 1).isdigit()]\n",
    "\n",
    "    if len(features) == 0:\n",
    "        print(f\"‚ö†Ô∏è No valid numeric features for {image_name}\")\n",
    "        return\n",
    "\n",
    "    # Dynamic normalization using mean ¬± 2*std\n",
    "    mean_val = np.mean(features)\n",
    "    std_val = np.std(features)\n",
    "    dynamic_min = max(0, mean_val - 2 * std_val)\n",
    "    dynamic_max = mean_val + 2 * std_val\n",
    "    normalized = np.clip(features, dynamic_min, dynamic_max)\n",
    "    normalized = (normalized - dynamic_min) / (dynamic_max - dynamic_min) * 255\n",
    "\n",
    "    # Apply sigmoid scaling\n",
    "    sigmoid_scaled = 255 / (1 + np.exp(-0.04 * (normalized - 127.5)))\n",
    "\n",
    "    # Pad to nearest square\n",
    "    total = len(sigmoid_scaled)\n",
    "    side = int(np.ceil(np.sqrt(total)))\n",
    "    padded = list(sigmoid_scaled) + [mean_val / (dynamic_max - dynamic_min) * 255] * (side * side - total)\n",
    "    reshaped = np.array(padded).reshape((side, side))\n",
    "\n",
    "    # Gaussian blur\n",
    "    blurred = gaussian(reshaped / 255.0, sigma=0.7) * 255\n",
    "\n",
    "    # CLAHE\n",
    "    clahe = equalize_adapthist(blurred / 255.0, clip_limit=0.03, kernel_size=10) * 255\n",
    "\n",
    "    # Sobel edge detection (slightly enhance edges for clarity)\n",
    "    edges = sobel(clahe / 255.0)\n",
    "    edge_enhanced = clahe * (1 + 0.5 * edges)\n",
    "    edge_enhanced = np.clip(edge_enhanced, 0, 255).astype(np.uint8)\n",
    "\n",
    "    # ‚úÖ Apply heatmap (using \"plasma\" for clear patterns)\n",
    "    norm = mcolors.Normalize(vmin=edge_enhanced.min(), vmax=edge_enhanced.max())\n",
    "    colored = cm.plasma(norm(edge_enhanced))[:, :, :3]   # take RGB\n",
    "    colored = (colored * 255).astype(np.uint8)\n",
    "    image = Image.fromarray(colored)\n",
    "\n",
    "    # Contrast and sharpening\n",
    "    image = ImageEnhance.Contrast(image).enhance(1.8)\n",
    "    image = image.filter(ImageFilter.UnsharpMask(radius=1.5, percent=200, threshold=2))\n",
    "\n",
    "    # Save the image\n",
    "    if not os.path.exists(output_folder_path):\n",
    "        os.makedirs(output_folder_path)\n",
    "\n",
    "    output_path = os.path.join(output_folder_path, image_name)\n",
    "    image.save(output_path)\n",
    "    print(f\"‚úÖ Saved Heatmap: {output_path} | Size: {side}x{side}\")\n",
    "\n",
    "# ---------- Main Code ----------\n",
    "\n",
    "input_csv = r'D:\\malware_LLm\\Malware_Benign_API_call_argument_Feature_Vector.csv'\n",
    "output_folder = r'D:\\malware_LLm\\Newheatmapout'\n",
    "\n",
    "with open(input_csv, 'r', encoding='utf-8') as file:\n",
    "    reader = csv.DictReader(file)\n",
    "    for row in reader:\n",
    "        generate_image_from_row(row, output_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24b6208-506a-485e-83fa-1b089f39d212",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"GPUs Available:\", tf.config.list_physical_devices('GPU'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a45c0d1-5c15-41d8-92fe-909e857bcd6d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# CNN for Heatmap Images (Training + Testing)\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import json\n",
    "import datetime\n",
    "\n",
    "# Constants\n",
    "IMAGE_SIZE = (128, 128)\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 50\n",
    "NUM_CLASSES = 8  # adware, backdoor, benign, downloader, spyware, trojan, virus, worm\n",
    "\n",
    "# Paths (update these for your dataset)\n",
    "data_dir = r'D:\\malware_LLm\\Newheatmapout'   # <-- folder jaha heatmap images hain\n",
    "results_dir = r'D:\\malware_LLm\\heatmapresult'\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "# Class names\n",
    "class_names = ['adware', 'backdoor', 'benign', 'downloader', 'spyware', 'trojan', 'virus', 'worm']\n",
    "\n",
    "# Dataset loader (for heatmap RGB images)\n",
    "def load_dataset_with_filenames(folder_path):\n",
    "    images = []\n",
    "    labels = []\n",
    "    filenames = []\n",
    "    \n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith('.png'):\n",
    "            class_name = filename.split('_')[0].lower()\n",
    "            if class_name in class_names:\n",
    "                img_path = os.path.join(folder_path, filename)\n",
    "                # Load as RGB for heatmap\n",
    "                img = load_img(img_path, color_mode='rgb', target_size=IMAGE_SIZE)\n",
    "                img_array = img_to_array(img) / 255.0\n",
    "                \n",
    "                images.append(img_array)\n",
    "                labels.append(class_name)\n",
    "                filenames.append(filename)\n",
    "    \n",
    "    return np.array(images), np.array(labels), np.array(filenames)\n",
    "\n",
    "# Load dataset\n",
    "print(\"Loading dataset with filenames...\")\n",
    "X, y, filenames = load_dataset_with_filenames(data_dir)\n",
    "print(f\"Loaded {len(X)} images\")\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "y_categorical = tf.keras.utils.to_categorical(y_encoded, num_classes=NUM_CLASSES)\n",
    "\n",
    "# Split data\n",
    "X_train, X_val, y_train, y_val, filenames_train, filenames_val = train_test_split(\n",
    "    X, y_categorical, filenames, test_size=0.2, random_state=42, stratify=y_encoded\n",
    ")\n",
    "\n",
    "# Data augmentation\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "train_generator = train_datagen.flow(\n",
    "    X_train, y_train, batch_size=BATCH_SIZE, shuffle=True\n",
    ")\n",
    "\n",
    "val_generator = ImageDataGenerator().flow(\n",
    "    X_val, y_val, batch_size=BATCH_SIZE, shuffle=False\n",
    ")\n",
    "\n",
    "# CNN Model for Heatmap RGB\n",
    "model = models.Sequential([\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(IMAGE_SIZE[0], IMAGE_SIZE[1], 3)),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(256, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(NUM_CLASSES, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train model with GPU\n",
    "print(\"Training model on GPU...\")\n",
    "with tf.device('/GPU:0'):\n",
    "    history = model.fit(\n",
    "        train_generator,\n",
    "        steps_per_epoch=len(X_train) // BATCH_SIZE,\n",
    "        epochs=EPOCHS,\n",
    "        validation_data=val_generator,\n",
    "        validation_steps=len(X_val) // BATCH_SIZE\n",
    "    )\n",
    "\n",
    "\n",
    "# Save results with timestamp\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "model_dir = os.path.join(results_dir, f\"model_{timestamp}\")\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save model + label encoder\n",
    "model.save(os.path.join(model_dir, 'malware_heatmap_cnn.keras'))\n",
    "np.save(os.path.join(model_dir, 'label_encoder.npy'), label_encoder.classes_)\n",
    "\n",
    "# Predictions\n",
    "print(\"Generating predictions...\")\n",
    "val_generator = ImageDataGenerator().flow(X_val, y_val, batch_size=BATCH_SIZE, shuffle=False)\n",
    "y_pred = model.predict(val_generator)\n",
    "predicted_classes = label_encoder.inverse_transform(np.argmax(y_pred, axis=1))\n",
    "true_classes = label_encoder.inverse_transform(np.argmax(y_val, axis=1))\n",
    "\n",
    "predictions_df = pd.DataFrame({\n",
    "    'image_name': filenames_val,\n",
    "    'actual_class': true_classes,\n",
    "    'predicted_class': predicted_classes\n",
    "})\n",
    "\n",
    "csv_path = os.path.join(model_dir, 'predictions.csv')\n",
    "predictions_df.to_csv(csv_path, index=False)\n",
    "print(f\"Predictions saved to {csv_path}\")\n",
    "\n",
    "# Save training artifacts\n",
    "def save_training_artifacts():\n",
    "    # Training history\n",
    "    with open(os.path.join(model_dir, 'training_history.json'), 'w') as f:\n",
    "        json.dump(history.history, f)\n",
    "    \n",
    "    # Model summary\n",
    "    with open(os.path.join(model_dir, 'model_summary.txt'), 'w', encoding='utf-8') as f:\n",
    "     model.summary(print_fn=lambda x: f.write(x + '\\n'))\n",
    "\n",
    "    \n",
    "    # Metrics\n",
    "    val_loss, val_acc = model.evaluate(val_generator, verbose=0)\n",
    "    metrics = {\n",
    "        'validation_accuracy': float(val_acc),\n",
    "        'validation_loss': float(val_loss),\n",
    "        'training_accuracy': float(history.history['accuracy'][-1]),\n",
    "        'training_loss': float(history.history['loss'][-1])\n",
    "    }\n",
    "    with open(os.path.join(model_dir, 'metrics.json'), 'w') as f:\n",
    "        json.dump(metrics, f, indent=4)\n",
    "    \n",
    "    # Training plots\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.title('Model Accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['loss'], label='Train Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Model Loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.savefig(os.path.join(model_dir, 'training_plots.png'))\n",
    "    plt.close()\n",
    "\n",
    "save_training_artifacts()\n",
    "\n",
    "print(f\"\\nAll results saved to: {model_dir}\")\n",
    "print(\"Sample predictions:\")\n",
    "print(predictions_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e0ac21-f478-4b1c-af44-de811a9e37e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available:\", len(tf.config.list_physical_devices('GPU')))\n",
    "print(\"Devices:\", tf.config.list_physical_devices())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f0f1c7-6c76-4b38-9b11-b73c0f26bddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"Num GPUs Available:\", len(tf.config.list_physical_devices('GPU')))\n",
    "print(\"Built with CUDA:\", tf.test.is_built_with_cuda())\n",
    "print(\"GPU Available:\", tf.test.is_gpu_available())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc09288-8983-4e18-a8c3-cba8df2c1a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"Num GPUs Available:\", len(tf.config.list_physical_devices('GPU')))\n",
    "print(\"GPU Details:\", tf.config.list_physical_devices('GPU'))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dcf6067-23fd-4da4-b813-53007e121726",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(\"TensorFlow:\", tf.__version__)\n",
    "print(\"GPUs:\", tf.config.list_physical_devices(\"GPU\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe48eda-1ac0-4cdb-8de0-dd0eeb880c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(\"TensorFlow:\", tf.__version__)\n",
    "print(\"GPUs:\", tf.config.list_physical_devices(\"GPU\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35116eed-bd12-45a0-b5a9-7464f844cfd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "CNN for Malware Heatmap Images\n",
    "---------------------------------\n",
    "This script:\n",
    "1. Loads heatmap images and preprocesses them\n",
    "2. Trains a Convolutional Neural Network (CNN)\n",
    "3. Saves predictions and evaluation metrics\n",
    "4. Generates plots, confusion matrix, and reports\n",
    "\"\"\"\n",
    "\n",
    "# =============================\n",
    "# Imports\n",
    "# =============================\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "import datetime\n",
    "\n",
    "# =============================\n",
    "# Configuration\n",
    "# =============================\n",
    "IMAGE_SIZE = (128, 128)\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 50\n",
    "NUM_CLASSES = 8\n",
    "CLASS_NAMES = ['adware', 'backdoor', 'benign', 'downloader', 'spyware', 'trojan', 'virus', 'worm']\n",
    "\n",
    "DATA_DIR = r\"D:\\malware_LLm\\Newheatmapout\"\n",
    "RESULTS_DIR = r\"D:\\malware_LLm\\heatmapresult\"\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "# =============================\n",
    "# Dataset Loader\n",
    "# =============================\n",
    "def load_dataset_with_filenames(folder_path):\n",
    "    \"\"\"Load heatmap images and return arrays of images, labels, and filenames\"\"\"\n",
    "    images, labels, filenames = [], [], []\n",
    "\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".png\"):\n",
    "            class_name = filename.split(\"_\")[0].lower()\n",
    "            if class_name in CLASS_NAMES:\n",
    "                img_path = os.path.join(folder_path, filename)\n",
    "                img = load_img(img_path, color_mode=\"rgb\", target_size=IMAGE_SIZE)\n",
    "                img_array = img_to_array(img) / 255.0\n",
    "\n",
    "                images.append(img_array)\n",
    "                labels.append(class_name)\n",
    "                filenames.append(filename)\n",
    "\n",
    "    return np.array(images), np.array(labels), np.array(filenames)\n",
    "\n",
    "print(\"Loading dataset...\")\n",
    "X, y, filenames = load_dataset_with_filenames(DATA_DIR)\n",
    "print(f\"Loaded {len(X)} images.\")\n",
    "\n",
    "# =============================\n",
    "# Label Encoding\n",
    "# =============================\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "y_categorical = tf.keras.utils.to_categorical(y_encoded, num_classes=NUM_CLASSES)\n",
    "\n",
    "# Train/Test Split\n",
    "X_train, X_val, y_train, y_val, filenames_train, filenames_val = train_test_split(\n",
    "    X, y_categorical, filenames, test_size=0.2, random_state=42, stratify=y_encoded\n",
    ")\n",
    "\n",
    "# =============================\n",
    "# Data Augmentation\n",
    "# =============================\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode=\"nearest\"\n",
    ")\n",
    "\n",
    "train_generator = train_datagen.flow(X_train, y_train, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_generator = ImageDataGenerator().flow(X_val, y_val, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# =============================\n",
    "# CNN Model\n",
    "# =============================\n",
    "model = models.Sequential([\n",
    "    layers.Conv2D(32, (3, 3), activation=\"relu\", input_shape=(IMAGE_SIZE[0], IMAGE_SIZE[1], 3)),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation=\"relu\"),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(128, (3, 3), activation=\"relu\"),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(256, activation=\"relu\"),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(NUM_CLASSES, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# =============================\n",
    "# Training\n",
    "# =============================\n",
    "print(\"Training model...\")\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=len(X_train) // BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=val_generator,\n",
    "    validation_steps=len(X_val) // BATCH_SIZE\n",
    ")\n",
    "\n",
    "# =============================\n",
    "# Save Model + Results Directory\n",
    "# =============================\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "model_dir = os.path.join(RESULTS_DIR, f\"model_{timestamp}\")\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "model.save(os.path.join(model_dir, \"malware_heatmap_cnn.keras\"))\n",
    "np.save(os.path.join(model_dir, \"label_encoder.npy\"), label_encoder.classes_)\n",
    "\n",
    "# =============================\n",
    "# Predictions\n",
    "# =============================\n",
    "print(\"Generating predictions...\")\n",
    "y_pred = model.predict(val_generator)\n",
    "predicted_classes = label_encoder.inverse_transform(np.argmax(y_pred, axis=1))\n",
    "true_classes = label_encoder.inverse_transform(np.argmax(y_val, axis=1))\n",
    "\n",
    "predictions_df = pd.DataFrame({\n",
    "    \"image_name\": filenames_val,\n",
    "    \"actual_class\": true_classes,\n",
    "    \"predicted_class\": predicted_classes\n",
    "})\n",
    "predictions_df.to_csv(os.path.join(model_dir, \"predictions.csv\"), index=False)\n",
    "\n",
    "# =============================\n",
    "# Evaluation: Confusion Matrix & Report\n",
    "# =============================\n",
    "cm = confusion_matrix(true_classes, predicted_classes, labels=CLASS_NAMES)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=CLASS_NAMES, yticklabels=CLASS_NAMES)\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.ylabel(\"Actual Class\")\n",
    "plt.xlabel(\"Predicted Class\")\n",
    "plt.savefig(os.path.join(model_dir, \"confusion_matrix.png\"))\n",
    "plt.close()\n",
    "\n",
    "# Classification Report\n",
    "report = classification_report(true_classes, predicted_classes, target_names=CLASS_NAMES, output_dict=True)\n",
    "report_df = pd.DataFrame(report).transpose()\n",
    "report_df.to_csv(os.path.join(model_dir, \"classification_report.csv\"))\n",
    "\n",
    "with open(os.path.join(model_dir, \"classification_report.txt\"), \"w\") as f:\n",
    "    f.write(classification_report(true_classes, predicted_classes, target_names=CLASS_NAMES))\n",
    "\n",
    "# =============================\n",
    "# Metrics\n",
    "# =============================\n",
    "val_loss, val_acc = model.evaluate(val_generator, verbose=0)\n",
    "metrics = {\n",
    "    \"validation_accuracy\": float(val_acc),\n",
    "    \"validation_loss\": float(val_loss),\n",
    "    \"training_accuracy\": float(history.history[\"accuracy\"][-1]),\n",
    "    \"training_loss\": float(history.history[\"loss\"][-1])\n",
    "}\n",
    "metrics_df = pd.DataFrame([metrics])\n",
    "metrics_df.to_csv(os.path.join(model_dir, \"metrics_results.csv\"), index=False)\n",
    "\n",
    "# Save Metrics Table as Image\n",
    "plt.figure(figsize=(6, 2))\n",
    "plt.axis(\"off\")\n",
    "table = plt.table(cellText=metrics_df.values, colLabels=metrics_df.columns, cellLoc=\"center\", loc=\"center\")\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(10)\n",
    "table.scale(1.2, 1.2)\n",
    "plt.savefig(os.path.join(model_dir, \"metrics_results_table.png\"), bbox_inches=\"tight\")\n",
    "plt.close()\n",
    "\n",
    "# =============================\n",
    "# Training Plots\n",
    "# =============================\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history[\"accuracy\"], label=\"Train Accuracy\", color=\"blue\")\n",
    "plt.plot(history.history[\"val_accuracy\"], label=\"Validation Accuracy\", color=\"orange\")\n",
    "plt.title(\"Training vs Validation Accuracy\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history[\"loss\"], label=\"Train Loss\", color=\"blue\")\n",
    "plt.plot(history.history[\"val_loss\"], label=\"Validation Loss\", color=\"orange\")\n",
    "plt.title(\"Training vs Validation Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "\n",
    "plt.savefig(os.path.join(model_dir, \"training_history.png\"))\n",
    "plt.close()\n",
    "\n",
    "# Save Training History JSON\n",
    "with open(os.path.join(model_dir, \"training_history.json\"), \"w\") as f:\n",
    "    json.dump(history.history, f)\n",
    "\n",
    "# Model Summary\n",
    "with open(os.path.join(model_dir, \"model_summary.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    model.summary(print_fn=lambda x: f.write(x + \"\\n\"))\n",
    "\n",
    "print(f\"\\n‚úÖ All results saved in: {model_dir}\")\n",
    "print(\"Sample predictions:\")\n",
    "print(predictions_df.head())\n",
    "\n",
    "# =============================\n",
    "# Final Combined Graph: Accuracy and Loss\n",
    "# =============================\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "epochs_range = range(1, EPOCHS + 1)\n",
    "\n",
    "# Plot Accuracy\n",
    "plt.plot(epochs_range, history.history[\"accuracy\"], label=\"Train Accuracy\", marker='o')\n",
    "plt.plot(epochs_range, history.history[\"val_accuracy\"], label=\"Validation Accuracy\", marker='o')\n",
    "\n",
    "# Plot Loss (scaled for better visualization)\n",
    "plt.plot(epochs_range, np.array(history.history[\"loss\"]) * 5, label=\"Train Loss (x5)\", linestyle=\"--\", marker='x')\n",
    "plt.plot(epochs_range, np.array(history.history[\"val_loss\"]) * 5, label=\"Validation Loss (x5)\", linestyle=\"--\", marker='x')\n",
    "\n",
    "plt.title(\"Training History: Accuracy and Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy / Loss (scaled)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.savefig(os.path.join(model_dir, \"combined_accuracy_loss.png\"))\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6fdd80-d104-4db2-8624-9ffbf8f8d7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "CNN for Malware Heatmap Images with Hyperparameter Tuning\n",
    "---------------------------------------------------------\n",
    "Steps:\n",
    "1. Loads heatmap images and preprocesses them\n",
    "2. Runs hyperparameter tuning using KerasTuner\n",
    "3. Selects the best CNN model\n",
    "4. Retrains best model with full epochs\n",
    "5. Saves predictions, metrics, and plots\n",
    "\"\"\"\n",
    "\n",
    "# =============================\n",
    "# Imports\n",
    "# =============================\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "import datetime\n",
    "import keras_tuner as kt\n",
    "\n",
    "# =============================\n",
    "# Configuration\n",
    "# =============================\n",
    "IMAGE_SIZE = (128, 128)\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 50\n",
    "NUM_CLASSES = 8\n",
    "CLASS_NAMES = ['adware', 'backdoor', 'benign', 'downloader', 'spyware', 'trojan', 'virus', 'worm']\n",
    "\n",
    "DATA_DIR = r\"D:\\malware_LLm\\Newheatmapout\"\n",
    "RESULTS_DIR = r\"D:\\malware_LLm\\heatmapresult\"\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "# =============================\n",
    "# Dataset Loader\n",
    "# =============================\n",
    "def load_dataset_with_filenames(folder_path):\n",
    "    images, labels, filenames = [], [], []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".png\"):\n",
    "            class_name = filename.split(\"_\")[0].lower()\n",
    "            if class_name in CLASS_NAMES:\n",
    "                img_path = os.path.join(folder_path, filename)\n",
    "                img = load_img(img_path, color_mode=\"rgb\", target_size=IMAGE_SIZE)\n",
    "                img_array = img_to_array(img) / 255.0\n",
    "                images.append(img_array)\n",
    "                labels.append(class_name)\n",
    "                filenames.append(filename)\n",
    "    return np.array(images), np.array(labels), np.array(filenames)\n",
    "\n",
    "print(\"Loading dataset...\")\n",
    "X, y, filenames = load_dataset_with_filenames(DATA_DIR)\n",
    "print(f\"Loaded {len(X)} images.\")\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "y_categorical = tf.keras.utils.to_categorical(y_encoded, num_classes=NUM_CLASSES)\n",
    "\n",
    "# Train/Test Split\n",
    "X_train, X_val, y_train, y_val, filenames_train, filenames_val = train_test_split(\n",
    "    X, y_categorical, filenames, test_size=0.2, random_state=42, stratify=y_encoded\n",
    ")\n",
    "\n",
    "# Data Augmentation\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode=\"nearest\"\n",
    ")\n",
    "train_generator = train_datagen.flow(X_train, y_train, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_generator = ImageDataGenerator().flow(X_val, y_val, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# =============================\n",
    "# Model Builder for Hyperparameter Tuning\n",
    "# =============================\n",
    "def build_model(hp):\n",
    "    model = models.Sequential()\n",
    "    \n",
    "    # Conv layers with tunable filters\n",
    "    model.add(layers.Conv2D(\n",
    "        filters=hp.Choice(\"conv1_filters\", [32, 64, 128]),\n",
    "        kernel_size=(3, 3),\n",
    "        activation=\"relu\",\n",
    "        input_shape=(IMAGE_SIZE[0], IMAGE_SIZE[1], 3)\n",
    "    ))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    \n",
    "    model.add(layers.Conv2D(\n",
    "        filters=hp.Choice(\"conv2_filters\", [64, 128, 256]),\n",
    "        kernel_size=(3, 3),\n",
    "        activation=\"relu\"\n",
    "    ))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    \n",
    "    model.add(layers.Conv2D(\n",
    "        filters=hp.Choice(\"conv3_filters\", [128, 256]),\n",
    "        kernel_size=(3, 3),\n",
    "        activation=\"relu\"\n",
    "    ))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    \n",
    "    # Flatten\n",
    "    model.add(layers.Flatten())\n",
    "    \n",
    "    # Dense layer with tunable units\n",
    "    model.add(layers.Dense(\n",
    "        units=hp.Choice(\"dense_units\", [128, 256, 512]),\n",
    "        activation=\"relu\"\n",
    "    ))\n",
    "    \n",
    "    # Dropout with tunable rate\n",
    "    model.add(layers.Dropout(hp.Choice(\"dropout_rate\", [0.3, 0.5, 0.7])))\n",
    "    \n",
    "    # Output\n",
    "    model.add(layers.Dense(NUM_CLASSES, activation=\"softmax\"))\n",
    "    \n",
    "    # Compile with tunable learning rate\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(\n",
    "            learning_rate=hp.Choice(\"learning_rate\", [1e-2, 1e-3, 1e-4])\n",
    "        ),\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# =============================\n",
    "# Hyperparameter Tuning\n",
    "# =============================\n",
    "tuner = kt.RandomSearch(\n",
    "    build_model,\n",
    "    objective=\"val_accuracy\",\n",
    "    max_trials=5,  # try more for deeper search\n",
    "    executions_per_trial=1,\n",
    "    directory=\"tuner_results\",\n",
    "    project_name=\"malware_heatmap\"\n",
    ")\n",
    "\n",
    "print(\"üîç Running hyperparameter search...\")\n",
    "tuner.search(\n",
    "    train_generator,\n",
    "    validation_data=val_generator,\n",
    "    epochs=10,   # short training for tuning\n",
    "    steps_per_epoch=len(X_train)//BATCH_SIZE,\n",
    "    validation_steps=len(X_val)//BATCH_SIZE\n",
    ")\n",
    "\n",
    "# Best model + hyperparameters\n",
    "best_model = tuner.get_best_models(num_models=1)[0]\n",
    "best_hps = tuner.get_best_hyperparameters(1)[0]\n",
    "print(\"‚úÖ Best Hyperparameters found:\")\n",
    "print(best_hps.values)\n",
    "\n",
    "# =============================\n",
    "# Retrain Best Model with Full Epochs\n",
    "# =============================\n",
    "print(\"üéØ Retraining best model...\")\n",
    "history = best_model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=len(X_train) // BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=val_generator,\n",
    "    validation_steps=len(X_val) // BATCH_SIZE\n",
    ")\n",
    "\n",
    "# =============================\n",
    "# Save Results\n",
    "# =============================\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "model_dir = os.path.join(RESULTS_DIR, f\"model_{timestamp}\")\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "best_model.save(os.path.join(model_dir, \"malware_heatmap_cnn_best.keras\"))\n",
    "np.save(os.path.join(model_dir, \"label_encoder.npy\"), label_encoder.classes_)\n",
    "\n",
    "# Predictions\n",
    "print(\"Generating predictions...\")\n",
    "y_pred = best_model.predict(val_generator)\n",
    "predicted_classes = label_encoder.inverse_transform(np.argmax(y_pred, axis=1))\n",
    "true_classes = label_encoder.inverse_transform(np.argmax(y_val, axis=1))\n",
    "predictions_df = pd.DataFrame({\n",
    "    \"image_name\": filenames_val,\n",
    "    \"actual_class\": true_classes,\n",
    "    \"predicted_class\": predicted_classes\n",
    "})\n",
    "predictions_df.to_csv(os.path.join(model_dir, \"predictions.csv\"), index=False)\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(true_classes, predicted_classes, labels=CLASS_NAMES)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=CLASS_NAMES, yticklabels=CLASS_NAMES)\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.ylabel(\"Actual Class\")\n",
    "plt.xlabel(\"Predicted Class\")\n",
    "plt.savefig(os.path.join(model_dir, \"confusion_matrix.png\"))\n",
    "plt.close()\n",
    "\n",
    "# Classification Report\n",
    "report = classification_report(true_classes, predicted_classes, target_names=CLASS_NAMES, output_dict=True)\n",
    "report_df = pd.DataFrame(report).transpose()\n",
    "report_df.to_csv(os.path.join(model_dir, \"classification_report.csv\"))\n",
    "with open(os.path.join(model_dir, \"classification_report.txt\"), \"w\") as f:\n",
    "    f.write(classification_report(true_classes, predicted_classes, target_names=CLASS_NAMES))\n",
    "\n",
    "# Metrics\n",
    "val_loss, val_acc = best_model.evaluate(val_generator, verbose=0)\n",
    "metrics = {\n",
    "    \"validation_accuracy\": float(val_acc),\n",
    "    \"validation_loss\": float(val_loss),\n",
    "    \"training_accuracy\": float(history.history[\"accuracy\"][-1]),\n",
    "    \"training_loss\": float(history.history[\"loss\"][-1])\n",
    "}\n",
    "metrics_df = pd.DataFrame([metrics])\n",
    "metrics_df.to_csv(os.path.join(model_dir, \"metrics_results.csv\"), index=False)\n",
    "\n",
    "# Training History JSON\n",
    "with open(os.path.join(model_dir, \"training_history.json\"), \"w\") as f:\n",
    "    json.dump(history.history, f)\n",
    "\n",
    "# Model Summary\n",
    "with open(os.path.join(model_dir, \"model_summary.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    best_model.summary(print_fn=lambda x: f.write(x + \"\\n\"))\n",
    "\n",
    "print(f\"\\n‚úÖ All results saved in: {model_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58749adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cbc1eabb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 5 Complete [00h 21m 40s]\n",
      "val_accuracy: 0.3914324641227722\n",
      "\n",
      "Best val_accuracy So Far: 0.8266092538833618\n",
      "Total elapsed time: 02h 41m 44s\n",
      "WARNING:tensorflow:From C:\\Users\\admin\\anaconda3\\envs\\arbaz\\lib\\site-packages\\keras\\src\\saving\\legacy\\save.py:538: The name tf.train.NewCheckpointReader is deprecated. Please use tf.compat.v1.train.NewCheckpointReader instead.\n",
      "\n",
      "‚úÖ Best Hyperparameters found:\n",
      "{'conv1_filters': 32, 'conv2_filters': 128, 'conv3_filters': 128, 'dense_units': 256, 'dropout_rate': 0.7, 'learning_rate': 0.0001}\n",
      "üéØ Retraining best model...\n",
      "Epoch 1/50\n",
      "552/552 [==============================] - 126s 227ms/step - loss: 0.6531 - accuracy: 0.7852 - val_loss: 0.6569 - val_accuracy: 0.7108\n",
      "Epoch 2/50\n",
      "552/552 [==============================] - 126s 228ms/step - loss: 0.6205 - accuracy: 0.7951 - val_loss: 0.6109 - val_accuracy: 0.8003\n",
      "Epoch 3/50\n",
      "552/552 [==============================] - 125s 226ms/step - loss: 0.6032 - accuracy: 0.8013 - val_loss: 0.4974 - val_accuracy: 0.8298\n",
      "Epoch 4/50\n",
      "552/552 [==============================] - 125s 227ms/step - loss: 0.5839 - accuracy: 0.8096 - val_loss: 0.5706 - val_accuracy: 0.8033\n",
      "Epoch 5/50\n",
      "552/552 [==============================] - 124s 225ms/step - loss: 0.5671 - accuracy: 0.8143 - val_loss: 0.4991 - val_accuracy: 0.8622\n",
      "Epoch 6/50\n",
      "552/552 [==============================] - 124s 224ms/step - loss: 0.5640 - accuracy: 0.8189 - val_loss: 0.4588 - val_accuracy: 0.8694\n",
      "Epoch 7/50\n",
      "552/552 [==============================] - 125s 226ms/step - loss: 0.5473 - accuracy: 0.8207 - val_loss: 0.4526 - val_accuracy: 0.8656\n",
      "Epoch 8/50\n",
      "552/552 [==============================] - 125s 226ms/step - loss: 0.5374 - accuracy: 0.8253 - val_loss: 0.4858 - val_accuracy: 0.8452\n",
      "Epoch 9/50\n",
      "552/552 [==============================] - 124s 224ms/step - loss: 0.5339 - accuracy: 0.8318 - val_loss: 0.4795 - val_accuracy: 0.8577\n",
      "Epoch 10/50\n",
      "552/552 [==============================] - 124s 225ms/step - loss: 0.5129 - accuracy: 0.8325 - val_loss: 0.4413 - val_accuracy: 0.8735\n",
      "Epoch 11/50\n",
      "552/552 [==============================] - 124s 224ms/step - loss: 0.5037 - accuracy: 0.8354 - val_loss: 0.4371 - val_accuracy: 0.8740\n",
      "Epoch 12/50\n",
      "552/552 [==============================] - 124s 225ms/step - loss: 0.5016 - accuracy: 0.8386 - val_loss: 0.4962 - val_accuracy: 0.8579\n",
      "Epoch 13/50\n",
      "552/552 [==============================] - 125s 226ms/step - loss: 0.4914 - accuracy: 0.8431 - val_loss: 0.3832 - val_accuracy: 0.8760\n",
      "Epoch 14/50\n",
      "552/552 [==============================] - 123s 224ms/step - loss: 0.4816 - accuracy: 0.8451 - val_loss: 0.3920 - val_accuracy: 0.8812\n",
      "Epoch 15/50\n",
      "552/552 [==============================] - 125s 226ms/step - loss: 0.4780 - accuracy: 0.8477 - val_loss: 0.3926 - val_accuracy: 0.8654\n",
      "Epoch 16/50\n",
      "552/552 [==============================] - 125s 225ms/step - loss: 0.4753 - accuracy: 0.8474 - val_loss: 0.4035 - val_accuracy: 0.8570\n",
      "Epoch 17/50\n",
      "552/552 [==============================] - 124s 224ms/step - loss: 0.4654 - accuracy: 0.8470 - val_loss: 0.3657 - val_accuracy: 0.8722\n",
      "Epoch 18/50\n",
      "552/552 [==============================] - 125s 226ms/step - loss: 0.4605 - accuracy: 0.8501 - val_loss: 0.4168 - val_accuracy: 0.8731\n",
      "Epoch 19/50\n",
      "552/552 [==============================] - 124s 224ms/step - loss: 0.4606 - accuracy: 0.8500 - val_loss: 0.3681 - val_accuracy: 0.8726\n",
      "Epoch 20/50\n",
      "552/552 [==============================] - 125s 226ms/step - loss: 0.4421 - accuracy: 0.8569 - val_loss: 0.3554 - val_accuracy: 0.8781\n",
      "Epoch 21/50\n",
      "552/552 [==============================] - 124s 225ms/step - loss: 0.4424 - accuracy: 0.8578 - val_loss: 0.3852 - val_accuracy: 0.8776\n",
      "Epoch 22/50\n",
      "552/552 [==============================] - 124s 225ms/step - loss: 0.4485 - accuracy: 0.8521 - val_loss: 0.3656 - val_accuracy: 0.8855\n",
      "Epoch 23/50\n",
      "552/552 [==============================] - 124s 225ms/step - loss: 0.4391 - accuracy: 0.8572 - val_loss: 0.3446 - val_accuracy: 0.8735\n",
      "Epoch 24/50\n",
      "552/552 [==============================] - 125s 226ms/step - loss: 0.4316 - accuracy: 0.8588 - val_loss: 0.3505 - val_accuracy: 0.8830\n",
      "Epoch 25/50\n",
      "552/552 [==============================] - 124s 225ms/step - loss: 0.4358 - accuracy: 0.8594 - val_loss: 0.3358 - val_accuracy: 0.8876\n",
      "Epoch 26/50\n",
      "552/552 [==============================] - 124s 225ms/step - loss: 0.4295 - accuracy: 0.8634 - val_loss: 0.3330 - val_accuracy: 0.8867\n",
      "Epoch 27/50\n",
      "552/552 [==============================] - 124s 224ms/step - loss: 0.4270 - accuracy: 0.8607 - val_loss: 0.3173 - val_accuracy: 0.8935\n",
      "Epoch 28/50\n",
      "552/552 [==============================] - 124s 225ms/step - loss: 0.4224 - accuracy: 0.8645 - val_loss: 0.3138 - val_accuracy: 0.8837\n",
      "Epoch 29/50\n",
      "552/552 [==============================] - 125s 226ms/step - loss: 0.4261 - accuracy: 0.8601 - val_loss: 0.3061 - val_accuracy: 0.8923\n",
      "Epoch 30/50\n",
      "552/552 [==============================] - 124s 225ms/step - loss: 0.4192 - accuracy: 0.8638 - val_loss: 0.3021 - val_accuracy: 0.8921\n",
      "Epoch 31/50\n",
      "552/552 [==============================] - 125s 226ms/step - loss: 0.4105 - accuracy: 0.8662 - val_loss: 0.3422 - val_accuracy: 0.8919\n",
      "Epoch 32/50\n",
      "552/552 [==============================] - 124s 225ms/step - loss: 0.4085 - accuracy: 0.8655 - val_loss: 0.3031 - val_accuracy: 0.8935\n",
      "Epoch 33/50\n",
      "552/552 [==============================] - 124s 224ms/step - loss: 0.3958 - accuracy: 0.8720 - val_loss: 0.2920 - val_accuracy: 0.8926\n",
      "Epoch 34/50\n",
      "552/552 [==============================] - 125s 226ms/step - loss: 0.4070 - accuracy: 0.8673 - val_loss: 0.2997 - val_accuracy: 0.8951\n",
      "Epoch 35/50\n",
      "552/552 [==============================] - 125s 226ms/step - loss: 0.4022 - accuracy: 0.8685 - val_loss: 0.3163 - val_accuracy: 0.8874\n",
      "Epoch 36/50\n",
      "552/552 [==============================] - 125s 227ms/step - loss: 0.3990 - accuracy: 0.8698 - val_loss: 0.2875 - val_accuracy: 0.8987\n",
      "Epoch 37/50\n",
      "552/552 [==============================] - 124s 225ms/step - loss: 0.3903 - accuracy: 0.8719 - val_loss: 0.2908 - val_accuracy: 0.8932\n",
      "Epoch 38/50\n",
      "552/552 [==============================] - 125s 226ms/step - loss: 0.3895 - accuracy: 0.8738 - val_loss: 0.2900 - val_accuracy: 0.9232\n",
      "Epoch 39/50\n",
      "552/552 [==============================] - 125s 227ms/step - loss: 0.3930 - accuracy: 0.8747 - val_loss: 0.2789 - val_accuracy: 0.9334\n",
      "Epoch 40/50\n",
      "552/552 [==============================] - 126s 228ms/step - loss: 0.3792 - accuracy: 0.8746 - val_loss: 0.2743 - val_accuracy: 0.9059\n",
      "Epoch 41/50\n",
      "552/552 [==============================] - 124s 224ms/step - loss: 0.3835 - accuracy: 0.8758 - val_loss: 0.2830 - val_accuracy: 0.8998\n",
      "Epoch 42/50\n",
      "552/552 [==============================] - 125s 226ms/step - loss: 0.3704 - accuracy: 0.8815 - val_loss: 0.2815 - val_accuracy: 0.8980\n",
      "Epoch 43/50\n",
      "552/552 [==============================] - 124s 225ms/step - loss: 0.3751 - accuracy: 0.8812 - val_loss: 0.2842 - val_accuracy: 0.8955\n",
      "Epoch 44/50\n",
      "552/552 [==============================] - 125s 227ms/step - loss: 0.3744 - accuracy: 0.8814 - val_loss: 0.2673 - val_accuracy: 0.9016\n",
      "Epoch 45/50\n",
      "552/552 [==============================] - 126s 228ms/step - loss: 0.3611 - accuracy: 0.8819 - val_loss: 0.2602 - val_accuracy: 0.9170\n",
      "Epoch 46/50\n",
      "552/552 [==============================] - 124s 225ms/step - loss: 0.3578 - accuracy: 0.8851 - val_loss: 0.2533 - val_accuracy: 0.9157\n",
      "Epoch 47/50\n",
      "552/552 [==============================] - 126s 228ms/step - loss: 0.3555 - accuracy: 0.8882 - val_loss: 0.2360 - val_accuracy: 0.9368\n",
      "Epoch 48/50\n",
      "552/552 [==============================] - 125s 226ms/step - loss: 0.3554 - accuracy: 0.8860 - val_loss: 0.2453 - val_accuracy: 0.9350\n",
      "Epoch 49/50\n",
      "552/552 [==============================] - 125s 226ms/step - loss: 0.3491 - accuracy: 0.8906 - val_loss: 0.2721 - val_accuracy: 0.9078\n",
      "Epoch 50/50\n",
      "552/552 [==============================] - 126s 229ms/step - loss: 0.3474 - accuracy: 0.8915 - val_loss: 0.2374 - val_accuracy: 0.9372\n",
      "Generating predictions...\n",
      "138/138 [==============================] - 5s 36ms/step\n",
      "\n",
      "‚úÖ All results saved in: D:\\malware_LLm\\heatmapresult\\model_20250919_230117\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "CNN for Malware Heatmap Images with Hyperparameter Tuning\n",
    "---------------------------------------------------------\n",
    "Steps:\n",
    "1. Loads heatmap images and preprocesses them\n",
    "2. Runs hyperparameter tuning using KerasTuner\n",
    "3. Selects the best CNN model\n",
    "4. Retrains best model with full epochs\n",
    "5. Saves predictions, metrics, and plots\n",
    "\"\"\"\n",
    "\n",
    "# =============================\n",
    "# Imports\n",
    "# =============================\n",
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "import datetime\n",
    "import keras_tuner as kt\n",
    "\n",
    "# =============================\n",
    "# Configuration\n",
    "# =============================\n",
    "IMAGE_SIZE = (128, 128)\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 50\n",
    "CLASS_NAMES = ['adware', 'backdoor', 'benign', 'downloader', 'spyware', 'trojan', 'virus', 'worm']\n",
    "\n",
    "DATA_DIR = r\"D:\\malware_LLm\\Newheatmapout\"\n",
    "RESULTS_DIR = r\"D:\\malware_LLm\\heatmapresult\"\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "# =============================\n",
    "# Dataset Loader\n",
    "# =============================\n",
    "def load_dataset_with_filenames(folder_path):\n",
    "    images, labels, filenames = [], [], []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.lower().endswith(\".png\"):\n",
    "            class_name = filename.split(\"_\")[0].lower()\n",
    "            if class_name in CLASS_NAMES:\n",
    "                img_path = os.path.join(folder_path, filename)\n",
    "                img = load_img(img_path, color_mode=\"rgb\", target_size=IMAGE_SIZE)\n",
    "                img_array = img_to_array(img) / 255.0\n",
    "                images.append(img_array)\n",
    "                labels.append(class_name)\n",
    "                filenames.append(filename)\n",
    "    return np.array(images), np.array(labels), np.array(filenames)\n",
    "\n",
    "print(\"Loading dataset...\")\n",
    "X, y, filenames = load_dataset_with_filenames(DATA_DIR)\n",
    "print(f\"Loaded {len(X)} images.\")\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "NUM_CLASSES = len(label_encoder.classes_)   # dynamic classes\n",
    "y_categorical = tf.keras.utils.to_categorical(y_encoded, num_classes=NUM_CLASSES)\n",
    "\n",
    "print(\"Class counts:\", dict(zip(*np.unique(y, return_counts=True))))\n",
    "print(\"NUM_CLASSES:\", NUM_CLASSES)\n",
    "\n",
    "# Train/Test Split\n",
    "X_train, X_val, y_train, y_val, filenames_train, filenames_val = train_test_split(\n",
    "    X, y_categorical, filenames, test_size=0.2, random_state=42, stratify=y_encoded\n",
    ")\n",
    "\n",
    "print(\"Train size:\", len(X_train), \"Val size:\", len(X_val))\n",
    "\n",
    "# Data Augmentation\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode=\"nearest\"\n",
    ")\n",
    "train_generator = train_datagen.flow(X_train, y_train, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_generator = ImageDataGenerator().flow(X_val, y_val, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# Safe steps\n",
    "steps_per_epoch = max(1, math.ceil(len(X_train) / BATCH_SIZE))\n",
    "validation_steps = max(1, math.ceil(len(X_val) / BATCH_SIZE))\n",
    "print(\"steps_per_epoch:\", steps_per_epoch, \"validation_steps:\", validation_steps)\n",
    "\n",
    "# =============================\n",
    "# Model Builder for Hyperparameter Tuning\n",
    "# =============================\n",
    "def build_model(hp):\n",
    "    model = models.Sequential()\n",
    "    \n",
    "    # Conv layers with tunable filters\n",
    "    model.add(layers.Conv2D(\n",
    "        filters=hp.Choice(\"conv1_filters\", [32, 64, 128]),\n",
    "        kernel_size=(3, 3),\n",
    "        activation=\"relu\",\n",
    "        input_shape=(IMAGE_SIZE[0], IMAGE_SIZE[1], 3)\n",
    "    ))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    \n",
    "    model.add(layers.Conv2D(\n",
    "        filters=hp.Choice(\"conv2_filters\", [64, 128, 256]),\n",
    "        kernel_size=(3, 3),\n",
    "        activation=\"relu\"\n",
    "    ))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    \n",
    "    model.add(layers.Conv2D(\n",
    "        filters=hp.Choice(\"conv3_filters\", [128, 256]),\n",
    "        kernel_size=(3, 3),\n",
    "        activation=\"relu\"\n",
    "    ))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    \n",
    "    # Flatten\n",
    "    model.add(layers.Flatten())\n",
    "    \n",
    "    # Dense layer with tunable units\n",
    "    model.add(layers.Dense(\n",
    "        units=hp.Choice(\"dense_units\", [128, 256, 512]),\n",
    "        activation=\"relu\"\n",
    "    ))\n",
    "    \n",
    "    # Dropout with tunable rate\n",
    "    model.add(layers.Dropout(hp.Choice(\"dropout_rate\", [0.3, 0.5, 0.7])))\n",
    "    \n",
    "    # Output\n",
    "    model.add(layers.Dense(NUM_CLASSES, activation=\"softmax\"))\n",
    "    \n",
    "    # Compile with tunable learning rate\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(\n",
    "            learning_rate=hp.Choice(\"learning_rate\", [1e-2, 1e-3, 1e-4])\n",
    "        ),\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# =============================\n",
    "# Hyperparameter Tuning\n",
    "# =============================\n",
    "tuner = kt.RandomSearch(\n",
    "    build_model,\n",
    "    objective=\"val_accuracy\",\n",
    "    max_trials=5,\n",
    "    executions_per_trial=1,\n",
    "    directory=\"tuner_results\",\n",
    "    project_name=\"malware_heatmap\"\n",
    ")\n",
    "\n",
    "print(\"üîç Running hyperparameter search...\")\n",
    "tuner.search(\n",
    "    train_generator,\n",
    "    validation_data=val_generator,\n",
    "    epochs=10,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    validation_steps=validation_steps\n",
    ")\n",
    "\n",
    "# Best model + hyperparameters\n",
    "best_model = tuner.get_best_models(num_models=1)[0]\n",
    "best_hps = tuner.get_best_hyperparameters(1)[0]\n",
    "print(\"‚úÖ Best Hyperparameters found:\")\n",
    "print(best_hps.values)\n",
    "\n",
    "# =============================\n",
    "# Retrain Best Model with Full Epochs\n",
    "# =============================\n",
    "print(\"üéØ Retraining best model...\")\n",
    "history = best_model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=val_generator,\n",
    "    validation_steps=validation_steps\n",
    ")\n",
    "\n",
    "# =============================\n",
    "# Save Results\n",
    "# =============================\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "model_dir = os.path.join(RESULTS_DIR, f\"model_{timestamp}\")\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "best_model.save(os.path.join(model_dir, \"malware_heatmap_cnn_best.keras\"))\n",
    "np.save(os.path.join(model_dir, \"label_encoder.npy\"), label_encoder.classes_)\n",
    "\n",
    "# Predictions\n",
    "print(\"Generating predictions...\")\n",
    "y_pred = best_model.predict(val_generator)\n",
    "predicted_classes = label_encoder.inverse_transform(np.argmax(y_pred, axis=1))\n",
    "true_classes = label_encoder.inverse_transform(np.argmax(y_val, axis=1))\n",
    "predictions_df = pd.DataFrame({\n",
    "    \"image_name\": filenames_val,\n",
    "    \"actual_class\": true_classes,\n",
    "    \"predicted_class\": predicted_classes\n",
    "})\n",
    "predictions_df.to_csv(os.path.join(model_dir, \"predictions.csv\"), index=False)\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(true_classes, predicted_classes, labels=CLASS_NAMES)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=CLASS_NAMES, yticklabels=CLASS_NAMES)\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.ylabel(\"Actual Class\")\n",
    "plt.xlabel(\"Predicted Class\")\n",
    "plt.savefig(os.path.join(model_dir, \"confusion_matrix.png\"))\n",
    "plt.close()\n",
    "\n",
    "# Classification Report\n",
    "report = classification_report(true_classes, predicted_classes, target_names=CLASS_NAMES, output_dict=True)\n",
    "report_df = pd.DataFrame(report).transpose()\n",
    "report_df.to_csv(os.path.join(model_dir, \"classification_report.csv\"))\n",
    "with open(os.path.join(model_dir, \"classification_report.txt\"), \"w\") as f:\n",
    "    f.write(classification_report(true_classes, predicted_classes, target_names=CLASS_NAMES))\n",
    "\n",
    "# Metrics\n",
    "val_loss, val_acc = best_model.evaluate(val_generator, verbose=0)\n",
    "metrics = {\n",
    "    \"validation_accuracy\": float(val_acc),\n",
    "    \"validation_loss\": float(val_loss),\n",
    "    \"training_accuracy\": float(history.history[\"accuracy\"][-1]),\n",
    "    \"training_loss\": float(history.history[\"loss\"][-1])\n",
    "}\n",
    "metrics_df = pd.DataFrame([metrics])\n",
    "metrics_df.to_csv(os.path.join(model_dir, \"metrics_results.csv\"), index=False)\n",
    "\n",
    "# Training History JSON\n",
    "with open(os.path.join(model_dir, \"training_history.json\"), \"w\") as f:\n",
    "    json.dump(history.history, f)\n",
    "\n",
    "# Model Summary\n",
    "with open(os.path.join(model_dir, \"model_summary.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    best_model.summary(print_fn=lambda x: f.write(x + \"\\n\"))\n",
    "\n",
    "print(f\"\\n‚úÖ All results saved in: {model_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d07131a-6182-459a-962d-69f36224cfa3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (arbaz)",
   "language": "python",
   "name": "arbaz"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
